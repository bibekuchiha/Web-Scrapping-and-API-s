# -*- coding: utf-8 -*-
"""kijiji_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SkL5zOvjDvNK_vWAT-JO7zliQBy5S_qw

The code you provided includes several Python libraries being imported into the current script using the import statement. Here is a brief explanation of each import statement:

requests library allows you to send HTTP/1.1 requests using Python, which means you can access webpages and retrieve data from them.

BeautifulSoup is a library for parsing HTML and XML documents, and is used here to extract data from HTML web pages.

time library is used here to call the sleep function, which pauses the script execution for a specified number of seconds, and helps prevent overwhelming the server between connections.

tqdm is a progress bar library used here to display a progress bar to show the status of a long-running process.

numpy is a numerical computing library that provides support for multi-dimensional arrays, matrices, and mathematical functions to manipulate them.

pandas is a library that provides data manipulation and analysis tools. In this code, it is used to wrangle data.

matplotlib is a plotting library used for creating static, interactive, and animated visualizations in Python.

plotly is an interactive visualization library that allows you to create and share interactive, web-based visualizations in Python.

The last two lines of the code set some options for Pandas to display data nicely when printing data frames.
"""

import requests
from bs4 import BeautifulSoup

# To prevent overwhelming the server between connections
from time import sleep 

# Display the progress bar
from tqdm import tqdm

# For data wrangling
import numpy as np
import pandas as pd

pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

# For creating plots
import matplotlib.pyplot as plt
import plotly.graph_objects as go

"""This code defines a function named get_page that takes three arguments: city, type, and page.

The purpose of the function is to retrieve and parse a web page from the Kijiji website based on the specified arguments. The function creates a URL using the arguments and sends an HTTP GET request to the URL using the requests.get() function. The response is then parsed using BeautifulSoup and the parsed content is returned.

The function also checks the HTTP status code of the response and prints a message based on the code. If the status code is in the range of 200 to 299, the function considers the response successful and proceeds to parse the HTML content of the page. Otherwise, it prints an appropriate error message.

The function returns the parsed content of the webpage as a BeautifulSoup object.
"""

def get_page(city, type,page):
  
  #  url = f'https://www.torontorentals.com/{city}/{type}?beds={beds}%20&p={page}'
    url = f"https://www.kijiji.ca/b-for-rent/{city}/{type}/page-{page}/k0c30349001l1700272?ad=offering&rb=true"
    # https://www.torontorentals.com/toronto/condos?beds=1%20&p=2
    
    result = requests.get(url)
    
    # check HTTP response status codes to find if HTTP request has been successfully completed
    if result.status_code >= 100  and result.status_code <= 199:
        print('Informational response')
    if result.status_code >= 200  and result.status_code <= 299:
        print('Successful response')
        soup = BeautifulSoup(result.content, "lxml")
    if result.status_code >= 300  and result.status_code <= 399:
        print('Redirect')
    if result.status_code >= 400  and result.status_code <= 499:
        print('Client error')
    if result.status_code >= 500  and result.status_code <= 599:
        print('Server error')
        
    return soup

"""This code creates an empty list named data and then runs a for loop that iterates over the range of numbers from 1 to 19 (inclusive).

For each iteration of the loop, the code first calls the sleep() function to pause the execution of the script for 2 seconds. This is to prevent overwhelming the server between connections.

The get_page() function is then called with the arguments 'toronto', 'rent', and 20, which returns a BeautifulSoup object containing the parsed content of a web page from the Kijiji website.

The code then uses soup_page.find_all() to find all the HTML elements on the page with the tag div and class attribute set to "info-container". These elements represent the rental listings on the page.

For each listing, the code uses list.find() to find specific HTML elements that contain information about the rental, such as the price, title, location, date posted, and description. The information is then stored in a list named info.

Finally, the info list is appended to the data list.

By the end of the loop, the data list contains a list of lists, where each inner list represents a rental listing and contains the rental's price, title, location, date posted, and description.
"""

data = []
for page_num in tqdm(range(1, 20)):
    sleep(2)
    
    # get soup object of the page
    # Find all the listings on the page

    soup_page = get_page('toronto', 'rent',20)
    # Find all the listings on the page
    lists = soup_page.find_all("div", {"class": "info-container"})

    for list in lists:
        # Get the price of the listing
        price = list.find('div', class_="price").text.strip()

        # Get the address of the listing
        title = list.find('div', class_='title').text.strip()

        # Get the number of bedrooms and bathrooms
        location = list.find('span', class_="").text.strip()
        date_posted = list.find('span', class_="date-posted").text.strip()
        description = list.find('div', class_="description").text.strip()

        info = [price, title, location, date_posted, description]
        data.append(info)

print(data)

"""This code uses the pandas library to create a DataFrame from the data list of lists. The pd.DataFrame() function is used to create the DataFrame, and the columns parameter is set to a list of column names: 'Price', 'Title', 'Location', 'Date Posted', and 'Description'.

The DataFrame is then saved to a CSV file named 'KijijiRentals.csv' using the df.to_csv() function. The index parameter is set to False to prevent the index column from being saved to the CSV file.

The resulting CSV file contains the data in tabular form, where each row represents a rental listing and contains the rental's price, title, location, date posted, and description.
"""

import pandas as pd

# Create a DataFrame from the data
df = pd.DataFrame(data, columns=['Price', 'Title', 'Location', 'Date Posted', 'Description'])

# Save the DataFrame to a CSV file
df.to_csv('KijijiRentals.csv', index=False)

import pandas as pd
df = pd.read_csv('/content/KijijiRentals.csv')

df.head()

df.shape

df['Location'].nunique()

df.isnull().sum()

df.info()

df.head()

df.head(20)

# remove the dollar sign and comma from the "Price" column
df['Price'] = df['Price'].str.replace('$', '').str.replace(',', '')

# replace "Please Contact" values with NaN
df['Price'] = pd.to_numeric(df['Price'], errors='coerce')

df.head(20)

df.isnull().sum()

# Remove leading/trailing spaces and line breaks
df['Location'] = df['Location'].str.strip().replace('\n','')

# Remove unnecessary text
df['Location'] = df['Location'].replace({'ago':''}, regex=True)
df['Location'] = df['Location'].replace({'Region':''}, regex=True)

df['Location'] = df['Location'].str.strip().str.replace('\n', '')

df['Location']

df.head(40)

df['Location'].nunique()

df['Location'].unique()

df.info()

df['Date Posted']

import seaborn as sns

# Distribution of prices
plt.figure(figsize=(10,6))
sns.histplot(df['Price'], bins=30)
plt.title('Distribution of Prices')
plt.show()

# get the value counts
Location_counts = df['Location'].value_counts()

# plot the value counts
# Plot bar chart
plt.figure(figsize=(12,8))
plt.bar(Location_counts.index, Location_counts.values)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('Location', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Kijiji Rentals by Location', fontsize=14)
plt.tight_layout()
plt.show()

"""We can see the maximum number of rent and housing are opened in Toronto and Brampton"""

# Average price by location
plt.figure(figsize=(12,6))
sns.barplot(x='Location', y='Price', data=df)
plt.xticks(rotation=90)
plt.title('Average Price by Location')
plt.show()

"""### Insights from the EDA
1. The distribution of prices is positively skewed, indicating that most of the posts are for lower priced listings.
2. The average price is highest in Oakville and lowest in Oshawa.

### NLP Work
"""

descriptions = df['Description'].tolist()

import spacy

nlp = spacy.load("en_core_web_sm")

for description in descriptions:

    # Tokenize the text
    doc = nlp(description)

    # Remove stop words and punctuation
    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]

    # Lemmatize the remaining words
    lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]

from textblob import TextBlob

# Create a TextBlob object from the preprocessed text
blob = TextBlob(" ".join(lemmas))

# Get the sentiment polarity (-1 to 1)
polarity = blob.sentiment.polarity

# Get the sentiment subjectivity (0 to 1)
subjectivity = blob.sentiment.subjectivity

print("Sentiment polarity:", polarity)
print("Sentiment subjectivity:", subjectivity)

from gensim import corpora, models

# Create a dictionary from the preprocessed text
dictionary = corpora.Dictionary([lemmas])

# Create a bag-of-words corpus from the dictionary
corpus = [dictionary.doc2bow(lemmas)]

# Perform topic modeling on the corpus
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)

# Print the main topics
for topic in lda_model.print_topics():
    print(topic)



Title = df['Title'].tolist()

nlp = spacy.load("en_core_web_sm")

for title in Title:

    # Tokenize the text
    doc = nlp(title)

    # Remove stop words and punctuation
    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]

    # Lemmatize the remaining words
    lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]

from textblob import TextBlob

# Create a TextBlob object from the preprocessed text
blob = TextBlob(" ".join(lemmas))

# Get the sentiment polarity (-1 to 1)
polarity = blob.sentiment.polarity

# Get the sentiment subjectivity (0 to 1)
subjectivity = blob.sentiment.subjectivity

print("Sentiment polarity:", polarity)
print("Sentiment subjectivity:", subjectivity)

from gensim import corpora, models

# Create a dictionary from the preprocessed text
dictionary = corpora.Dictionary([lemmas])
dictionary.save('dictionary.dict')

# Create a bag-of-words corpus from the dictionary
corpus = [dictionary.doc2bow(lemmas)]

# Perform topic modeling on the corpus
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)
lda_model.save('lda_model.model')
# Print the main topics
for topic in lda_model.print_topics():
    print(topic)

# Perform named entity recognition on the preprocessed text
entities = [(entity.text, entity.label_) for entity in doc.ents]

# Extract location entities
locations = [entity[0] for entity in entities if entity[1] == "GPE"]

# Extract date entities
dates = [entity[0] for entity in entities if entity[1] == "DATE"]

print("Locations:", locations)
print("Dates:", dates)

import pyLDAvis.gensim_models
import gensim

# Load your LDA model and dictionary
lda_model = gensim.models.LdaModel.load('lda_model.model')
dictionary = gensim.corpora.Dictionary.load('dictionary.dict')

# Generate the visualization
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)

# Display the visualization
pyLDAvis.display(vis)

